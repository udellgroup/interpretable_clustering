{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tree Clusters Pipeline\n",
    "\n",
    "This code will build the supervised tree clusters to minimize distance between samples in each node. In order to run this code you will need to: \n",
    "1. Create your own pre_process function to read in your dataset and return the following dataframes: xTrain, xTest, yTrain, yTest, df_train, df_test (the last 2 are the full dataframes for training and testing) \n",
    "2. Run all cells below \n",
    "3. Call get_tree_clust(depth, pre_process_func) on the depth you want for your tree and the pre_process function that you just created. Your resulting tree will be printed and we will return the ROC AUC score for the tree and the resulting tree itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sc\n",
    "import sklearn \n",
    "\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "import itertools\n",
    "import gc\n",
    "import re\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def pre_process(ts=0.3):\n",
    "    titanic = pd.read_csv('train.csv')\n",
    "    #titanic\n",
    "    full_data = titanic\n",
    "    full_data = full_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    full_data['Cabin'] = full_data['Cabin'].fillna(\"U0\")\n",
    "    full_data['Deck'] = full_data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    full_data['Deck'] = full_data['Deck'].map(deck)\n",
    "    full_data['Deck'] = full_data['Deck'].fillna(0)\n",
    "    full_data['Deck'] = full_data['Deck'].astype(int)\n",
    "\n",
    "    full_data = full_data.drop('Cabin', axis = 1)\n",
    "\n",
    "    mean = full_data[\"Age\"].mean()\n",
    "    std = full_data[\"Age\"].std()\n",
    "    is_null = full_data[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = full_data[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    full_data[\"Age\"] = age_slice\n",
    "    full_data[\"Age\"] = full_data[\"Age\"].astype(int)\n",
    "    full_data[\"Age\"].isnull().sum()\n",
    "    full_data['Embarked'] = full_data['Embarked'].fillna('S')\n",
    "\n",
    "\n",
    "    full_data['Fare'] = full_data['Fare'].fillna(0)\n",
    "    full_data['Fare'] = full_data['Fare'].astype(int)\n",
    "    full_data = full_data.drop(['Name'], axis=1)\n",
    "    full_data = full_data.drop(['Ticket'], axis=1)\n",
    "    full_data['Sex'] = full_data['Sex'].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = ts,random_state = 10)\n",
    "\n",
    "    full_data = pd.get_dummies(full_data,columns = ['Pclass','Embarked','Deck'])\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('Survived',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['Survived']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = ts,random_state = 10)\n",
    "    full_data_train = full_data_train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    full_data_test = full_data_test[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=1000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_heart():\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    full_data = heart\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('target',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['target']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(G):\n",
    "    nV = len(G)\n",
    "    distance = list(map(lambda i: list(map(lambda j: j, i)), G))\n",
    "\n",
    "    # Adding vertices individually\n",
    "    for k in range(nV):\n",
    "        for i in range(nV):\n",
    "            for j in range(nV):\n",
    "                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_dists(estimator):\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "\n",
    "    dists = np.zeros((n_nodes,n_nodes)) \n",
    "    for i in range(len(children_left)):\n",
    "        left_node_id = children_left[i]\n",
    "        if left_node_id != -1:\n",
    "            dists[i][left_node_id] = 1\n",
    "            dists[left_node_id][i] = 1\n",
    "\n",
    "    for i in range(len(children_right)):\n",
    "        if children_right[i] != -1:\n",
    "            dists[i][children_right[i]] = 1\n",
    "            dists[children_right[i]][i] = 1\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(n_nodes):\n",
    "            if i != j and dists[i][j] == 0:\n",
    "                dists[i][j] = 1000 # equivalent to infinity because we need distances between ALL ndoes\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_fw(xTrain,yTrain,xTest, md):\n",
    "    train = xTrain.copy()\n",
    "    train['y'] = yTrain\n",
    "    train1 = train.sample(n = len(train), replace = True) \n",
    "    train1=train1.dropna(how='any')\n",
    "    yTrain1 = train1['y']\n",
    "    xTrain1 = train1.drop('y',axis = 1)\n",
    "    gc.collect()\n",
    "    \n",
    "    estimator = DecisionTreeClassifier(max_depth=md).fit(xTrain1,yTrain1)\n",
    "    leaves_train = estimator.apply(xTrain)\n",
    "    leaves_test = estimator.apply(xTest)\n",
    "    train_comb = list(itertools.combinations(range(0,len(xTrain)), 2))\n",
    "    test_comb = list(itertools.combinations(range(0,len(xTest)), 2))\n",
    "    graph = get_tree_dists(estimator)\n",
    "    fw_dist = floyd_warshall(graph)\n",
    "    \n",
    "    i1_train = [i for i, _ in train_comb]\n",
    "    i2_train = [i for _, i in train_comb]\n",
    "    train_dists = [fw_dist[leaves_train[i]][leaves_train[j]] for i, j in train_comb]\n",
    "    train_dist_df = pd.DataFrame(i1_train,columns=['i1'])\n",
    "    train_dist_df['i2'] = i2_train\n",
    "    train_dist_df['tree_dist'] = train_dists\n",
    "    \n",
    "    i1_test = [i for i, _ in test_comb]\n",
    "    i2_test = [i for _, i in test_comb]\n",
    "    test_dists = [fw_dist[leaves_test[i]][leaves_test[j]] for i, j in test_comb]\n",
    "    test_dist_df = pd.DataFrame(i1_test,columns=['i1'])\n",
    "    test_dist_df['i2'] = i2_test\n",
    "    test_dist_df['tree_dist'] = test_dists\n",
    "    \n",
    "    return([estimator,train_dist_df, test_dist_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(xTrain,yTrain,num_trees,xTest, md):\n",
    "    i = 0\n",
    "    mods = []\n",
    "    train_dists = pd.DataFrame()\n",
    "    test_dists = pd.DataFrame()\n",
    "    while i <= num_trees:\n",
    "        #print(yTrain)\n",
    "        tree = build_tree_fw(xTrain,yTrain,xTest, md)\n",
    "        mods.append(tree[0])\n",
    "        train_dists = train_dists.append(tree[1])\n",
    "        test_dists = test_dists.append(tree[2])\n",
    "        i = i+1\n",
    "        \n",
    "    train_final_dist = train_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "    test_final_dist = test_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "    return(mods,train_final_dist,test_final_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(xTest,mods):\n",
    "    pred = []\n",
    "    for clf in mods:\n",
    "        pred.append(clf.predict(xTest))\n",
    "    pred = np.mean(pred,axis = 0)\n",
    "    pred = [int(x) for x in pred>=0.5]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset, df):\n",
    "    feature = df.columns[index]\n",
    "    left, right = list(), list()\n",
    "    indleft, indright = list(), list()\n",
    "    left_df = df[df[feature] < value]\n",
    "    right_df = df[df[feature] >= value]\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "            indleft.append(i)\n",
    "        else:\n",
    "            right.append(row)\n",
    "            indright.append(i)\n",
    "    #print(indleft)\n",
    "    return left, right, indleft, indright, left_df, right_df\n",
    " \n",
    "# Calculate the distance index for a split dataset\n",
    "def distance_index(left, right, indleft, indright, train_final_dist):\n",
    "    df_left = train_final_dist[train_final_dist['i1'].isin(indleft)]\n",
    "    df_left = df_left[df_left['i2'].isin(indleft)]\n",
    "    mean_left_dist = df_left['tree_dist'].sum()\n",
    "    df_right = train_final_dist[train_final_dist['i1'].isin(indright)]\n",
    "    df_right = df_right[df_right['i2'].isin(indright)]\n",
    "    mean_right_dist = df_right['tree_dist'].sum()\n",
    "    \n",
    "    left_len = df_left.shape[0]\n",
    "    right_len = df_right.shape[0]\n",
    "    left_index = ((mean_left_dist + 1) / (left_len + 1)) * ((left_len+1) / (left_len + right_len + 1))\n",
    "    right_index = ((mean_right_dist+1) / (right_len + 1)) * ((right_len+1) / (left_len + right_len + 1))\n",
    "    \n",
    "    return (left_index + right_index) / 2\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, df, train_final_dist):\n",
    "    class_values = list(set(row[0] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(1, len(dataset[0])):\n",
    "        for row in dataset:\n",
    "            left, right, indleft, indright, left_df, right_df = test_split(index, row[index], dataset, df)\n",
    "            groups = left, right, left_df, right_df\n",
    "            dist = distance_index(left, right, indleft, indright, train_final_dist)\n",
    "            if dist < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], dist, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[0] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, train_final_dist):\n",
    "    left, right, left_df, right_df = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        #print('here')\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, left_df, train_final_dist)\n",
    "        split(node['left'], max_depth, min_size, depth+1, train_final_dist)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        #print('here')\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        #print('here')\n",
    "        node['right'] = get_split(right, right_df, train_final_dist)\n",
    "        split(node['right'], max_depth, min_size, depth+1, train_final_dist)\n",
    "# Build a decision tree\n",
    "def build_tree(train, df, max_depth, min_size, train_final_dist):\n",
    "    root = get_split(train, df, train_final_dist)\n",
    "    split(root, max_depth, min_size, 1, train_final_dist)\n",
    "    return root\n",
    " \n",
    "# Print a decision tree\n",
    "def print_tree(node, df, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth*' ', (df.columns[node['index']]), node['value'])))\n",
    "        print_tree(node['left'], df, depth+1)\n",
    "        print_tree(node['right'], df, depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_lst(tree_full, dataset_test):\n",
    "    y_pred_dt = []\n",
    "    for row in dataset_test:\n",
    "        y_pred_dt.append(predict(tree_full, row))\n",
    "    return y_pred_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Defne your own pre_process() function to return the dataframes: xTrain, xTest, yTrain, yTest, df_train, and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_clust(depth, pre_process_func):\n",
    "    xTrain, xTest, yTrain, yTest, df_train, df_test = pre_process_func\n",
    "    dataset_train = df_train.to_numpy()\n",
    "    dataset_test = df_test.to_numpy()\n",
    "    mods,train_final_dist,test_final_dist = fit_random_forest(xTrain,yTrain,100,xTest, depth)\n",
    "    train_final_dist = train_final_dist.groupby(['i1','i2']).mean().reset_index()\n",
    "    train_final_dist = train_final_dist[['i1', 'i2', 'tree_dist']]\n",
    "    tree = build_tree(dataset_train, df_train, depth, 30, train_final_dist)\n",
    "    ypred_dt = get_pred_lst(tree, dataset_test)\n",
    "    score = sklearn.metrics.roc_auc_score(yTest,ypred_dt)\n",
    "    print_tree(tree, df_train)\n",
    "    return tree, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sex < 1.000]\n",
      " [Age < 39.000]\n",
      "  [Fare < 27.000]\n",
      "   [Age < 22.000]\n",
      "    [Age < 19.000]\n",
      "     [Fare < 12.000]\n",
      "      [0]\n",
      "      [1]\n",
      "     [Age < 20.000]\n",
      "      [0]\n",
      "      [0]\n",
      "    [Fare < 13.000]\n",
      "     [Age < 25.000]\n",
      "      [Age < 23.000]\n",
      "       [0]\n",
      "       [0]\n",
      "      [Age < 28.000]\n",
      "       [0]\n",
      "       [0]\n",
      "     [Age < 35.000]\n",
      "      [Deck < 8.000]\n",
      "       [1]\n",
      "       [0]\n",
      "      [0]\n",
      "   [Fare < 135.000]\n",
      "    [Parch < 2.000]\n",
      "     [Deck < 4.000]\n",
      "      [0]\n",
      "      [0]\n",
      "     [0]\n",
      "    [0]\n",
      "  [Fare < 13.000]\n",
      "   [Age < 44.000]\n",
      "    [0]\n",
      "    [0]\n",
      "   [Pclass < 3.000]\n",
      "    [Parch < 1.000]\n",
      "     [Age < 42.000]\n",
      "      [0]\n",
      "      [Fare < 27.000]\n",
      "       [0]\n",
      "       [0]\n",
      "     [0]\n",
      "    [0]\n",
      " [Parch < 2.000]\n",
      "  [Fare < 8.000]\n",
      "   [Age < 21.000]\n",
      "    [1]\n",
      "    [1]\n",
      "   [Fare < 14.000]\n",
      "    [Deck < 8.000]\n",
      "     [1]\n",
      "     [Pclass < 3.000]\n",
      "      [1]\n",
      "      [0]\n",
      "    [Fare < 20.000]\n",
      "     [1]\n",
      "     [Age < 52.000]\n",
      "      [SibSp < 1.000]\n",
      "       [1]\n",
      "       [1]\n",
      "      [1]\n",
      "  [Age < 36.000]\n",
      "   [SibSp < 3.000]\n",
      "    [1]\n",
      "    [0]\n",
      "   [0]\n",
      "0.7718879921741257\n"
     ]
    }
   ],
   "source": [
    "tree, score = get_tree_clust(7, pre_process())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
