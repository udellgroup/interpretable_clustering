{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tree Clusters Pipeline\n",
    "\n",
    "This code will build the supervised tree clusters to minimize distance between samples in each node. In order to run this code you will need to: \n",
    "1. Create your own pre_process function to read in your dataset and return the following dataframes: xTrain, xTest, yTrain, yTest, df_train, df_test (the last 2 are the full dataframes for training and testing) \n",
    "2. Run all cells below \n",
    "3. Call get_tree_clust(depth, pre_process_func) on the depth you want for your tree and the pre_process function that you just created. Your resulting tree will be printed and we will return the ROC AUC score for the tree and the resulting tree itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/aparnacalambur/Documents/Cornell/Classes/MEng/interpretable_clustering', '', '/Library/Python/3.6/site-packages', '/Users/aparnacalambur/Documents/Cornell/Classes/MEng/interpretable_clustering', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/aparnacalambur/Library/Python/3.7/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/extensions', '/Users/aparnacalambur/.ipython', '/Users/aparnacalambur/Documents/Cornell']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import scipy.stats as sc\n",
    "# import shap\n",
    "# import lime\n",
    "import sklearn \n",
    "import warnings\n",
    "#import xgboost\n",
    "import itertools\n",
    "import gc\n",
    "import networkx as nx\n",
    "import pydot\n",
    "\n",
    "import multiprocessing as mp\n",
    "from itertools import product \n",
    "import time\n",
    "import pickle \n",
    "#import interpret\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "# from interpret.glassbox import ExplainableBoostingClassifier\n",
    "# from interpret import show\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from interpret.glassbox import ExplainableBoostingRegressor\n",
    "# from interpret import show\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "#import statsmodels.api as sm\n",
    "sys.path.append(os.path.abspath(\"../../../\"))\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"/anaconda3/lib/python3.6/site-packages\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def pre_process(ts=0.3):\n",
    "    titanic = pd.read_csv('train.csv')\n",
    "    #titanic\n",
    "    full_data = titanic\n",
    "    full_data = full_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    full_data['Cabin'] = full_data['Cabin'].fillna(\"U0\")\n",
    "    full_data['Deck'] = full_data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    full_data['Deck'] = full_data['Deck'].map(deck)\n",
    "    full_data['Deck'] = full_data['Deck'].fillna(0)\n",
    "    full_data['Deck'] = full_data['Deck'].astype(int)\n",
    "\n",
    "    full_data = full_data.drop('Cabin', axis = 1)\n",
    "\n",
    "    mean = full_data[\"Age\"].mean()\n",
    "    std = full_data[\"Age\"].std()\n",
    "    is_null = full_data[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = full_data[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    full_data[\"Age\"] = age_slice\n",
    "    full_data[\"Age\"] = full_data[\"Age\"].astype(int)\n",
    "    full_data[\"Age\"].isnull().sum()\n",
    "    full_data['Embarked'] = full_data['Embarked'].fillna('S')\n",
    "\n",
    "\n",
    "    full_data['Fare'] = full_data['Fare'].fillna(0)\n",
    "    full_data['Fare'] = full_data['Fare'].astype(int)\n",
    "    full_data = full_data.drop(['Name'], axis=1)\n",
    "    full_data = full_data.drop(['Ticket'], axis=1)\n",
    "    full_data['Sex'] = full_data['Sex'].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = ts,random_state = 10)\n",
    "\n",
    "    full_data = pd.get_dummies(full_data,columns = ['Pclass','Embarked','Deck'])\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('Survived',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['Survived']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = ts,random_state = 10)\n",
    "    full_data_train = full_data_train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    full_data_test = full_data_test[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_synthetic_non_linear():\n",
    "    n = 300\n",
    "    X = np.random.randn(n, 10)\n",
    "    y = -2 * np.sin(2*X[:,0]*X[:,2] ) + np.maximum(X[:,1], 0)  + np.exp(-X[:,3]) + np.random.randn(n)\n",
    "    cols = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "    X = preprocessing.scale(X)\n",
    "    y = preprocessing.scale(y)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    c = ['y'] + cols\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    full_data['y'] = y\n",
    "    for col in cols:\n",
    "        full_data[col] = X[col]\n",
    "    \n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=1000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult2():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=2500, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult3():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=5000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult4():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=10000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult5():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=20000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult6():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=40000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_heart():\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    #full_data = heart\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    heart_no_tar = heart.drop(columns = ['target'])\n",
    "    c = ['target'] + list(heart_no_tar.columns)\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        full_data[col] = heart[col]\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('target',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['target']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def floyd_warshall(G):\n",
    "#     nV = len(G)\n",
    "#     distance = list(map(lambda i: list(map(lambda j: j, i)), G))\n",
    "#     n = 0\n",
    "\n",
    "#     # Adding vertices individually\n",
    "#     for k in range(nV):\n",
    "#         for i in range(nV):\n",
    "#             for j in range(nV):\n",
    "#                 distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])\n",
    "#                 n += 1\n",
    "#     return distance, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tree_dists(estimator):\n",
    "#     n_nodes = estimator.tree_.node_count\n",
    "#     children_left = estimator.tree_.children_left\n",
    "#     children_right = estimator.tree_.children_right\n",
    "\n",
    "#     dists = np.empty((n_nodes,n_nodes)); dists.fill(10000)\n",
    "#     for i in range(len(children_left)):\n",
    "#         left_node_id = children_left[i]\n",
    "#         if left_node_id != -1:\n",
    "#             dists[i][left_node_id] = 1\n",
    "#             dists[left_node_id][i] = 1\n",
    "\n",
    "#     for i in range(len(children_right)):\n",
    "#         if children_right[i] != -1:\n",
    "#             dists[i][children_right[i]] = 1\n",
    "#             dists[children_right[i]][i] = 1\n",
    "#     return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_tree_fw(xTrain,yTrain,xTest, md):\n",
    "#     train = xTrain.copy()\n",
    "#     train['y'] = yTrain\n",
    "#     train1 = train.sample(n = len(train), replace = True) \n",
    "#     train1=train1.dropna(how='any')\n",
    "#     yTrain1 = train1['y']\n",
    "#     xTrain1 = train1.drop('y',axis = 1)\n",
    "#     gc.collect()\n",
    "    \n",
    "#     estimator = DecisionTreeClassifier(max_depth=md).fit(xTrain1,yTrain1)\n",
    "#     leaves_train = estimator.apply(xTrain)\n",
    "#     leaves_test = estimator.apply(xTest)\n",
    "#     train_comb = list(itertools.combinations(range(0,len(xTrain)), 2))\n",
    "#     #test_comb = list(itertools.combinations(range(0,len(xTest)), 2))\n",
    "#     graph = get_tree_dists(estimator)\n",
    "#     fw_dist, n = floyd_warshall(graph)\n",
    "    \n",
    "#     i1_train = [i for i, _ in train_comb]\n",
    "#     i2_train = [i for _, i in train_comb]\n",
    "#     train_dists = [fw_dist[leaves_train[i]][leaves_train[j]] for i, j in train_comb]\n",
    "#     train_dist_df = pd.DataFrame(i1_train,columns=['i1'])\n",
    "#     train_dist_df['i2'] = i2_train\n",
    "#     train_dist_df['tree_dist'] = train_dists\n",
    "    \n",
    "# #     i1_test = [i for i, _ in test_comb]\n",
    "# #     i2_test = [i for _, i in test_comb]\n",
    "# #     test_dists = [fw_dist[leaves_test[i]][leaves_test[j]] for i, j in test_comb]\n",
    "# #     test_dist_df = pd.DataFrame(i1_test,columns=['i1'])\n",
    "# #     test_dist_df['i2'] = i2_test\n",
    "# #     test_dist_df['tree_dist'] = test_dists\n",
    "    \n",
    "#     return([estimator,train_dist_df, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_random_forest(xTrain,yTrain,num_trees,xTest, md):\n",
    "#     i = 0\n",
    "#     mods = []\n",
    "#     nodes = 0\n",
    "#     train_dists = pd.DataFrame()\n",
    "#    # test_dists = pd.DataFrame()\n",
    "#     while i <= num_trees:\n",
    "#         #print(yTrain)\n",
    "#         tree = build_tree_fw(xTrain,yTrain,xTest, md)\n",
    "#         mods.append(tree[0])\n",
    "#         train_dists = train_dists.append(tree[1])\n",
    "#         nodes += (tree[2])\n",
    "#         #test_dists = test_dists.append(tree[2])\n",
    "#         i = i+1\n",
    "        \n",
    "#     train_final_dist = train_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "#    # test_final_dist = test_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "#     return(mods,train_final_dist, nodes, nodes/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rf_predict(xTest,mods):\n",
    "#     pred = []\n",
    "#     for clf in mods:\n",
    "#         pred.append(clf.predict(xTest))\n",
    "#     pred = np.mean(pred,axis = 0)\n",
    "#     pred = [int(x) for x in pred>=0.5]\n",
    "#     return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(tree1, X):\n",
    "    n_nodes = tree1.tree_.node_count\n",
    "    children_left = tree1.tree_.children_left\n",
    "    children_right = tree1.tree_.children_right\n",
    "    feature = tree1.tree_.feature\n",
    "    threshold = tree1.tree_.threshold\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    leaves = [i for i, x in enumerate(is_leaves) if x]\n",
    "    leaves = [str(l) for l in leaves]\n",
    "    leaf_pairs = list(itertools.combinations(leaves,2))\n",
    "    #print(type(tree1))\n",
    "    uMG = dt_to_mg(tree1)\n",
    "    res = []\n",
    "    for inds in leaf_pairs:\n",
    "        source = inds[0]\n",
    "        target = inds[1]\n",
    "        distance = nx.shortest_path_length(uMG, source=inds[0], target=inds[1])\n",
    "        res.append([(source+','+target),distance])\n",
    "    results = pd.DataFrame(res,columns = ['key','dist'])\n",
    "\n",
    "    leaf_id = tree1.apply(X)\n",
    "    instance_pairs = list(itertools.combinations(range(0,len(leaf_id)),2))\n",
    "    instance_dist = pd.DataFrame(instance_pairs,columns = ['i1','i2'])\n",
    "    instance_dist['l1'] = leaf_id[instance_dist['i1'].values]\n",
    "    instance_dist['l2'] = leaf_id[instance_dist['i2'].values]\n",
    "    instance_dist['leaf1'] = instance_dist[['l1','l2']].min(axis = 1)\n",
    "    instance_dist['leaf2'] = instance_dist[['l1','l2']].max(axis = 1)\n",
    "    instance_dist = instance_dist.drop(['l1','l2'], axis = 1)\n",
    "    instance_dist['key'] = instance_dist['leaf1'].astype(str) + ',' + instance_dist['leaf2'].astype(str)\n",
    "    all_distances = pd.merge(instance_dist,results,on = 'key', how = 'left')\n",
    "    all_distances = all_distances.fillna(0)\n",
    "\n",
    "    return all_distances[['i1','i2','dist']], len(leaves)\n",
    "\n",
    "def forest_distances(rf,X):\n",
    "    alldist = pd.DataFrame()\n",
    "    total_nodes = 0\n",
    "    for tree1 in rf.estimators_:\n",
    "        dist, num_nodes = compute_distances(tree1, X)\n",
    "        total_nodes += num_nodes\n",
    "        alldist = alldist.append(dist)\n",
    "    alldist_agg = alldist.groupby(['i1','i2']).dist.mean().reset_index()\n",
    "    \n",
    "    s = max(max(alldist_agg['i1']), max(alldist_agg['i2'])) + 1\n",
    "    distmatrix = np.zeros(shape = (s,s))\n",
    "    for i,row in alldist_agg.iterrows():\n",
    "        i1 = int(row['i1'])\n",
    "        i2 = int(row['i2'])\n",
    "        distmatrix[i1][i2] = row['dist']\n",
    "        distmatrix[i2][i1] = row['dist']\n",
    "\n",
    "    return distmatrix, total_nodes\n",
    "\n",
    "def dt_to_mg(clf):\n",
    "    dot_data = tree.export_graphviz(clf)\n",
    "    dot_graph = pydot.graph_from_dot_data(dot_data)[0]\n",
    "    MG = nx.drawing.nx_pydot.from_pydot(dot_graph)\n",
    "    uMG = MG.to_undirected()\n",
    "    return uMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset, df):\n",
    "    feature = df.columns[index]\n",
    "    left, right = list(), list()\n",
    "    indleft, indright = list(), list()\n",
    "    left_df = df[df[feature] < value]\n",
    "    right_df = df[df[feature] >= value]\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "            indleft.append(i)\n",
    "        else:\n",
    "            right.append(row)\n",
    "            indright.append(i)\n",
    "    #print(indleft)\n",
    "    return left, right, indleft, indright, left_df, right_df\n",
    " \n",
    "# Calculate the avg distance for a split dataset\n",
    "def distance_index(indleft, indright, ind_tup_dct, train_final_dist_dct):\n",
    "    left_len = len(indleft)\n",
    "    right_len = len(indright)\n",
    "    mean_left_dist = 0\n",
    "    mean_right_dist = 0\n",
    "    if left_len > 0:\n",
    "        resultL = ind_tup_dct[indleft[0]]\n",
    "        for i in indleft[:-1]:\n",
    "            resultL = resultL.intersection(ind_tup_dct[i])\n",
    "        for k in resultL:\n",
    "            mean_left_dist += train_final_dist_dct[k]\n",
    "    if right_len > 0:\n",
    "        resultR = ind_tup_dct[indright[0]]\n",
    "        for j in indright[:-1]:\n",
    "            resultR = resultR.intersection(ind_tup_dct[j])\n",
    "        for l in resultR:\n",
    "            mean_right_dist += train_final_dist_dct[l]\n",
    "\n",
    "    \n",
    "    left_avg = (mean_left_dist / left_len) * (left_len / (left_len + right_len)) if left_len > 0 else 0\n",
    "    right_avg = (mean_right_dist / right_len) * (right_len / (left_len + right_len)) if right_len > 0 else 0\n",
    "    \n",
    "    return left_avg + right_avg\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, df, ind_tup_dct, train_final_dist_dct):\n",
    "    class_values = list(set(row[0] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(1, len(dataset[0])):\n",
    "        for row in dataset:\n",
    "            left, right, indleft, indright, left_df, right_df = test_split(index, row[index], dataset, df)\n",
    "            groups = left, right, left_df, right_df\n",
    "            dist = distance_index(indleft, indright, ind_tup_dct, train_final_dist_dct)\n",
    "            if dist < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], dist, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[0] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, ind_tup_dct, train_final_dist_dct):\n",
    "    left, right, left_df, right_df = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        #print('here')\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, left_df, ind_tup_dct, train_final_dist_dct)\n",
    "        split(node['left'], max_depth, min_size, depth+1, ind_tup_dct, train_final_dist_dct)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        #print('here')\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        #print('here')\n",
    "        node['right'] = get_split(right, right_df, ind_tup_dct, train_final_dist_dct)\n",
    "        split(node['right'], max_depth, min_size, depth+1, ind_tup_dct, train_final_dist_dct)\n",
    "# Build a decision tree\n",
    "def build_tree(train, df, max_depth, min_size, ind_tup_dct, train_final_dist_dct):\n",
    "    root = get_split(train, df, ind_tup_dct, train_final_dist_dct)\n",
    "    split(root, max_depth, min_size, 1, ind_tup_dct, train_final_dist_dct)\n",
    "    return root\n",
    " \n",
    "# Print a decision tree\n",
    "def print_tree(node, df, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth*' ', (df.columns[node['index']]), node['value'])))\n",
    "        print_tree(node['left'], df, depth+1)\n",
    "        print_tree(node['right'], df, depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_lst(tree_full, dataset_test):\n",
    "    y_pred_dt = []\n",
    "    for row in dataset_test:\n",
    "        y_pred_dt.append(predict(tree_full, row))\n",
    "    return y_pred_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Defne your own pre_process() function to return the dataframes: xTrain, xTest, yTrain, yTest, df_train, and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_lst = []\n",
    "num_dim_lst = []\n",
    "depth_lst = []\n",
    "time_lst = []\n",
    "num_nodes = []\n",
    "mean_nodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocs = [pre_processing_adult(), pre_processing_adult2(), pre_processing_adult3(), pre_processing_adult4(), pre_processing_adult5(), pre_processing_adult6()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_clust(depth, pre_process_func):\n",
    "    xTrain, xTest, yTrain, yTest, df_train, df_test = pre_process_func\n",
    "    dataset_train = df_train.to_numpy()\n",
    "    dataset_test = df_test.to_numpy()\n",
    "    rf = RandomForestClassifier(n_estimators = 20, max_depth = depth).fit(xTrain,yTrain)\n",
    "    distances, total_nodes = forest_distances(rf,xTrain)\n",
    "    num_nodes.append(total_nodes)\n",
    "    inds = [i for i in range(0, len(distances))] \n",
    "    tuples = list(product(inds, inds))\n",
    "    dist = [j for sub in distances for j in sub]\n",
    "    train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "    ind_tup_dct = {}\n",
    "    for i, j in tuples:\n",
    "        if i in ind_tup_dct.keys():\n",
    "            ind_tup_dct[i].add((i, j))\n",
    "        else:\n",
    "            ind_tup_dct[i] = set()\n",
    "            ind_tup_dct[i].add((i, j))\n",
    "        if j in ind_tup_dct.keys():\n",
    "            ind_tup_dct[j].add((i, j))\n",
    "        else:\n",
    "            ind_tup_dct[j] = set()\n",
    "            ind_tup_dct[j].add((i, j))\n",
    "    tree = build_tree(dataset_train, df_train, depth, 30, ind_tup_dct, train_final_dist_dct)\n",
    "    #ypred_dt = get_pred_lst(tree, dataset_test)\n",
    "    #score = sklearn.metrics.roc_auc_score(yTest,ypred_dt)\n",
    "    #print_tree(tree, df_train)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n"
     ]
    }
   ],
   "source": [
    "for fn in preprocs:\n",
    "    print(\"New Dataset\")\n",
    "    num_rows = fn[0].shape[0] + fn[1].shape[0]\n",
    "    num_dim = fn[0].shape[1]\n",
    "    \n",
    "    for depth in range(1, 6):\n",
    "        print(\"new depth\")\n",
    "        start = time.time()\n",
    "        tree_full_test_1 = get_tree_clust(depth, fn)\n",
    "        end = time.time()\n",
    "        num_rows_lst.append(num_rows)\n",
    "        num_dim_lst.append(num_dim)\n",
    "        depth_lst.append(depth)\n",
    "        time_lst.append(end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['num_rows', 'num_cols', 'tree_depth', 'num_nodes\", ''time'])\n",
    "df['num_rows'] = num_rows_lst\n",
    "df['num_cols'] = num_dim_lst\n",
    "df['tree_depth'] = depth_lst\n",
    "df['num_nodes'] = num_nodes\n",
    "df['time'] = time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best python debugging tool - python debugger effiecny (code profiler)\n",
    "# num rep nodes vs. time (also depth/num rows vs. rep nodes)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_processing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ae0f33d52433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_processing' is not defined"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cf93b77214af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distance Time: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6003544ece3b>\u001b[0m in \u001b[0;36mforest_distances\u001b[0;34m(rf, X)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mdistmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mdistmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4376\u001b[0m         \u001b[0;31m# use this, e.g. DatetimeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m         \u001b[0;31m# Things like `Series._get_value` (via .at) pass the EA directly here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4378\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mextract_array\u001b[0;34m(obj, extract_numpy)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \"\"\"\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCIndexClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__instancecheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing_adult2()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))\n",
    "\n",
    "start2 = time.time()\n",
    "inds = [i for i in range(0, len(distances))] \n",
    "tuples = list(product(inds, inds))\n",
    "dist = [j for sub in distances for j in sub]\n",
    "train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "ind_tup_dct = {}\n",
    "for i, j in tuples:\n",
    "    if i in ind_tup_dct.keys():\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[i] = set()\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    if j in ind_tup_dct.keys():\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[j] = set()\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "tree_full_test_1 = build_tree(dataset_train, df_train, 5, 30, ind_tup_dct, train_final_dist_dct)\n",
    "end2 = time.time()\n",
    "print(\"Cluster Time: \"+ str(end2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing_adult3()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))\n",
    "\n",
    "start2 = time.time()\n",
    "inds = [i for i in range(0, len(distances))] \n",
    "tuples = list(product(inds, inds))\n",
    "dist = [j for sub in distances for j in sub]\n",
    "train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "ind_tup_dct = {}\n",
    "for i, j in tuples:\n",
    "    if i in ind_tup_dct.keys():\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[i] = set()\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    if j in ind_tup_dct.keys():\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[j] = set()\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "tree_full_test_1 = build_tree(dataset_train, df_train, 5, 30, ind_tup_dct, train_final_dist_dct)\n",
    "end2 = time.time()\n",
    "print(\"Cluster Time: \"+ str(end2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.lib.c_is_list_like'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/abc.py\", line 137, in __instancecheck__\n",
      "    def __instancecheck__(cls, instance):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Time: 35946.87841510773\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing_adult4()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))\n",
    "\n",
    "start2 = time.time()\n",
    "inds = [i for i in range(0, len(distances))] \n",
    "tuples = list(product(inds, inds))\n",
    "dist = [j for sub in distances for j in sub]\n",
    "train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "ind_tup_dct = {}\n",
    "for i, j in tuples:\n",
    "    if i in ind_tup_dct.keys():\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[i] = set()\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    if j in ind_tup_dct.keys():\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[j] = set()\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "tree_full_test_1 = build_tree(dataset_train, df_train, 5, 30, ind_tup_dct, train_final_dist_dct)\n",
    "end2 = time.time()\n",
    "print(\"Cluster Time: \"+ str(end2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing_adult5()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))\n",
    "\n",
    "start2 = time.time()\n",
    "inds = [i for i in range(0, len(distances))] \n",
    "tuples = list(product(inds, inds))\n",
    "dist = [j for sub in distances for j in sub]\n",
    "train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "ind_tup_dct = {}\n",
    "for i, j in tuples:\n",
    "    if i in ind_tup_dct.keys():\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[i] = set()\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    if j in ind_tup_dct.keys():\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[j] = set()\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "tree_full_test_1 = build_tree(dataset_train, df_train, 5, 30, ind_tup_dct, train_final_dist_dct)\n",
    "end2 = time.time()\n",
    "print(\"Cluster Time: \"+ str(end2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_processing_adult6()\n",
    "start = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 20, max_depth = 5).fit(xTrain,yTrain)\n",
    "distances = forest_distances(rf,xTrain)\n",
    "end = time.time()\n",
    "print(\"Distance Time: \"+ str(end-start))\n",
    "\n",
    "start2 = time.time()\n",
    "inds = [i for i in range(0, len(distances))] \n",
    "tuples = list(product(inds, inds))\n",
    "dist = [j for sub in distances for j in sub]\n",
    "train_final_dist_dct = dict(zip(tuples, dist)) \n",
    "ind_tup_dct = {}\n",
    "for i, j in tuples:\n",
    "    if i in ind_tup_dct.keys():\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[i] = set()\n",
    "        ind_tup_dct[i].add((i, j))\n",
    "    if j in ind_tup_dct.keys():\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "    else:\n",
    "        ind_tup_dct[j] = set()\n",
    "        ind_tup_dct[j].add((i, j))\n",
    "\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "tree_full_test_1 = build_tree(dataset_train, df_train, 5, 30, ind_tup_dct, train_final_dist_dct)\n",
    "end2 = time.time()\n",
    "print(\"Cluster Time: \"+ str(end2-start2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
