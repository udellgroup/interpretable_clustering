{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tree Clusters Pipeline\n",
    "\n",
    "This code will build the supervised tree clusters to minimize distance between samples in each node. In order to run this code you will need to: \n",
    "1. Create your own pre_process function to read in your dataset and return the following dataframes: xTrain, xTest, yTrain, yTest, df_train, df_test (the last 2 are the full dataframes for training and testing) \n",
    "2. Run all cells below \n",
    "3. Call get_tree_clust(depth, pre_process_func) on the depth you want for your tree and the pre_process function that you just created. Your resulting tree will be printed and we will return the ROC AUC score for the tree and the resulting tree itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sc\n",
    "import sklearn \n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "import itertools\n",
    "import gc\n",
    "import re\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def pre_process(ts=0.3):\n",
    "    titanic = pd.read_csv('train.csv')\n",
    "    #titanic\n",
    "    full_data = titanic\n",
    "    full_data = full_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    full_data['Cabin'] = full_data['Cabin'].fillna(\"U0\")\n",
    "    full_data['Deck'] = full_data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    full_data['Deck'] = full_data['Deck'].map(deck)\n",
    "    full_data['Deck'] = full_data['Deck'].fillna(0)\n",
    "    full_data['Deck'] = full_data['Deck'].astype(int)\n",
    "\n",
    "    full_data = full_data.drop('Cabin', axis = 1)\n",
    "\n",
    "    mean = full_data[\"Age\"].mean()\n",
    "    std = full_data[\"Age\"].std()\n",
    "    is_null = full_data[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = full_data[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    full_data[\"Age\"] = age_slice\n",
    "    full_data[\"Age\"] = full_data[\"Age\"].astype(int)\n",
    "    full_data[\"Age\"].isnull().sum()\n",
    "    full_data['Embarked'] = full_data['Embarked'].fillna('S')\n",
    "\n",
    "\n",
    "    full_data['Fare'] = full_data['Fare'].fillna(0)\n",
    "    full_data['Fare'] = full_data['Fare'].astype(int)\n",
    "    full_data = full_data.drop(['Name'], axis=1)\n",
    "    full_data = full_data.drop(['Ticket'], axis=1)\n",
    "    full_data['Sex'] = full_data['Sex'].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = ts,random_state = 10)\n",
    "\n",
    "    full_data = pd.get_dummies(full_data,columns = ['Pclass','Embarked','Deck'])\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('Survived',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['Survived']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = ts,random_state = 10)\n",
    "    full_data_train = full_data_train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    full_data_test = full_data_test[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_synthetic_non_linear():\n",
    "    n = 300\n",
    "    X = np.random.randn(n, 10)\n",
    "    y = -2 * np.sin(2*X[:,0]*X[:,2] ) + np.maximum(X[:,1], 0)  + np.exp(-X[:,3]) + np.random.randn(n)\n",
    "    cols = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "    X = preprocessing.scale(X)\n",
    "    y = preprocessing.scale(y)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    c = ['y'] + cols\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    full_data['y'] = y\n",
    "    for col in cols:\n",
    "        full_data[col] = X[col]\n",
    "    \n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=1000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult2():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=2500, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult3():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=5000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult4():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=10000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult5():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=20000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult6():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=40000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_heart():\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    #full_data = heart\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    heart_no_tar = heart.drop(columns = ['target'])\n",
    "    c = ['target'] + list(heart_no_tar.columns)\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        full_data[col] = heart[col]\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('target',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['target']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(G):\n",
    "    nV = len(G)\n",
    "    distance = list(map(lambda i: list(map(lambda j: j, i)), G))\n",
    "\n",
    "    # Adding vertices individually\n",
    "    for k in range(nV):\n",
    "        for i in range(nV):\n",
    "            for j in range(nV):\n",
    "                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_dists(estimator):\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "\n",
    "    dists = np.zeros((n_nodes,n_nodes)) \n",
    "    for i in range(len(children_left)):\n",
    "        left_node_id = children_left[i]\n",
    "        if left_node_id != -1:\n",
    "            dists[i][left_node_id] = 1\n",
    "            dists[left_node_id][i] = 1\n",
    "\n",
    "    for i in range(len(children_right)):\n",
    "        if children_right[i] != -1:\n",
    "            dists[i][children_right[i]] = 1\n",
    "            dists[children_right[i]][i] = 1\n",
    "\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(n_nodes):\n",
    "            if i != j and dists[i][j] == 0:\n",
    "                dists[i][j] = 1000 # equivalent to infinity because we need distances between ALL ndoes\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_fw(xTrain,yTrain,xTest, md):\n",
    "    train = xTrain.copy()\n",
    "    train['y'] = yTrain\n",
    "    train1 = train.sample(n = len(train), replace = True) \n",
    "    train1=train1.dropna(how='any')\n",
    "    yTrain1 = train1['y']\n",
    "    xTrain1 = train1.drop('y',axis = 1)\n",
    "    gc.collect()\n",
    "    \n",
    "    estimator = DecisionTreeClassifier(max_depth=md).fit(xTrain1,yTrain1)\n",
    "    leaves_train = estimator.apply(xTrain)\n",
    "    leaves_test = estimator.apply(xTest)\n",
    "    train_comb = list(itertools.combinations(range(0,len(xTrain)), 2))\n",
    "    test_comb = list(itertools.combinations(range(0,len(xTest)), 2))\n",
    "    graph = get_tree_dists(estimator)\n",
    "    fw_dist = floyd_warshall(graph)\n",
    "    \n",
    "    i1_train = [i for i, _ in train_comb]\n",
    "    i2_train = [i for _, i in train_comb]\n",
    "    train_dists = [fw_dist[leaves_train[i]][leaves_train[j]] for i, j in train_comb]\n",
    "    train_dist_df = pd.DataFrame(i1_train,columns=['i1'])\n",
    "    train_dist_df['i2'] = i2_train\n",
    "    train_dist_df['tree_dist'] = train_dists\n",
    "    \n",
    "    i1_test = [i for i, _ in test_comb]\n",
    "    i2_test = [i for _, i in test_comb]\n",
    "    test_dists = [fw_dist[leaves_test[i]][leaves_test[j]] for i, j in test_comb]\n",
    "    test_dist_df = pd.DataFrame(i1_test,columns=['i1'])\n",
    "    test_dist_df['i2'] = i2_test\n",
    "    test_dist_df['tree_dist'] = test_dists\n",
    "    \n",
    "    return([estimator,train_dist_df, test_dist_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(xTrain,yTrain,num_trees,xTest, md):\n",
    "    i = 0\n",
    "    mods = []\n",
    "    train_dists = pd.DataFrame()\n",
    "    test_dists = pd.DataFrame()\n",
    "    while i <= num_trees:\n",
    "        #print(yTrain)\n",
    "        tree = build_tree_fw(xTrain,yTrain,xTest, md)\n",
    "        mods.append(tree[0])\n",
    "        train_dists = train_dists.append(tree[1])\n",
    "        test_dists = test_dists.append(tree[2])\n",
    "        i = i+1\n",
    "        \n",
    "    train_final_dist = train_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "    test_final_dist = test_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "    return(mods,train_final_dist,test_final_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(xTest,mods):\n",
    "    pred = []\n",
    "    for clf in mods:\n",
    "        pred.append(clf.predict(xTest))\n",
    "    pred = np.mean(pred,axis = 0)\n",
    "    pred = [int(x) for x in pred>=0.5]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset, df):\n",
    "    feature = df.columns[index]\n",
    "    left, right = list(), list()\n",
    "    indleft, indright = list(), list()\n",
    "    left_df = df[df[feature] < value]\n",
    "    right_df = df[df[feature] >= value]\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "            indleft.append(i)\n",
    "        else:\n",
    "            right.append(row)\n",
    "            indright.append(i)\n",
    "    #print(indleft)\n",
    "    return left, right, indleft, indright, left_df, right_df\n",
    " \n",
    "# Calculate the avg distance for a split dataset\n",
    "def distance_calc(indleft, indright, train_final_dist):\n",
    "    df_left = train_final_dist[train_final_dist['i1'].isin(indleft)]\n",
    "    df_left = df_left[df_left['i2'].isin(indleft)]\n",
    "    mean_left_dist = df_left['tree_dist'].sum()\n",
    "    df_right = train_final_dist[train_final_dist['i1'].isin(indright)]\n",
    "    df_right = df_right[df_right['i2'].isin(indright)]\n",
    "    mean_right_dist = df_right['tree_dist'].sum()\n",
    "    \n",
    "    left_len = df_left.shape[0]\n",
    "    right_len = df_right.shape[0]\n",
    "    \n",
    "    left_avg = (mean_left_dist / left_len) * (left_len / (left_len + right_len)) if left_len > 0 else 0\n",
    "    right_avg = (mean_right_dist / right_len) * (right_len / (left_len + right_len)) if right_len > 0 else 0\n",
    "    \n",
    "    return left_avg+ right_avg\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, df, train_final_dist):\n",
    "    class_values = list(set(row[0] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(1, len(dataset[0])):\n",
    "        for row in dataset:\n",
    "            left, right, indleft, indright, left_df, right_df = test_split(index, row[index], dataset, df)\n",
    "            groups = left, right, left_df, right_df\n",
    "            dist = distance_index(left, right, indleft, indright, train_final_dist)\n",
    "            if dist < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], dist, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[0] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, train_final_dist):\n",
    "    left, right, left_df, right_df = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        #print('here')\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, left_df, train_final_dist)\n",
    "        split(node['left'], max_depth, min_size, depth+1, train_final_dist)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        #print('here')\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        #print('here')\n",
    "        node['right'] = get_split(right, right_df, train_final_dist)\n",
    "        split(node['right'], max_depth, min_size, depth+1, train_final_dist)\n",
    "# Build a decision tree\n",
    "def build_tree(train, df, max_depth, min_size, train_final_dist):\n",
    "    root = get_split(train, df, train_final_dist)\n",
    "    split(root, max_depth, min_size, 1, train_final_dist)\n",
    "    return root\n",
    " \n",
    "# Print a decision tree\n",
    "def print_tree(node, df, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth*' ', (df.columns[node['index']]), node['value'])))\n",
    "        print_tree(node['left'], df, depth+1)\n",
    "        print_tree(node['right'], df, depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_lst(tree_full, dataset_test):\n",
    "    y_pred_dt = []\n",
    "    for row in dataset_test:\n",
    "        y_pred_dt.append(predict(tree_full, row))\n",
    "    return y_pred_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Defne your own pre_process() function to return the dataframes: xTrain, xTest, yTrain, yTest, df_train, and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_lst = []\n",
    "num_dim_lst = []\n",
    "depth_lst = []\n",
    "time_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocs = [pre_processing_adult(), pre_processing_adult2(), pre_processing_adult3(), pre_processing_adult4(), pre_processing_adult5(),pre_processing_adult6()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_clust(depth, pre_process_func):\n",
    "    xTrain, xTest, yTrain, yTest, df_train, df_test = pre_process_func\n",
    "    dataset_train = df_train.to_numpy()\n",
    "    dataset_test = df_test.to_numpy()\n",
    "    mods,train_final_dist,test_final_dist = fit_random_forest(xTrain,yTrain,100,xTest, depth)\n",
    "    train_final_dist = train_final_dist.groupby(['i1','i2']).mean().reset_index()\n",
    "    train_final_dist = train_final_dist[['i1', 'i2', 'tree_dist']]\n",
    "    tree = build_tree(dataset_train, df_train, depth, 30, train_final_dist)\n",
    "    #ypred_dt = get_pred_lst(tree, dataset_test)\n",
    "    #score = sklearn.metrics.roc_auc_score(yTest,ypred_dt)\n",
    "    #print_tree(tree, df_train)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tree_dist(depth, pre_process_func):\n",
    "    xTrain, xTest, yTrain, yTest, df_train, df_test = pre_process_func\n",
    "    dataset_train = df_train.to_numpy()\n",
    "    dataset_test = df_test.to_numpy()\n",
    "    mods,train_final_dist,test_final_dist = fit_random_forest(xTrain,yTrain,100,xTest, depth)\n",
    "    train_final_dist = train_final_dist.groupby(['i1','i2']).mean().reset_index()\n",
    "    train_final_dist = train_final_dist[['i1', 'i2', 'tree_dist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d0eaabee508b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tree_clust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnum_rows_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8d10f9f2eaa0>\u001b[0m in \u001b[0;36mget_tree_clust\u001b[0;34m(depth, pre_process_func)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_final_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'i2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_final_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#ypred_dt = get_pred_lst(tree, dataset_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#score = sklearn.metrics.roc_auc_score(yTest,ypred_dt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0103971fc1a4>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(train, df, max_depth, min_size, train_final_dist)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Build a decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0103971fc1a4>\u001b[0m in \u001b[0;36mget_split\u001b[0;34m(dataset, df, train_final_dist)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_final_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mb_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0103971fc1a4>\u001b[0m in \u001b[0;36mdistance_index\u001b[0;34m(left, right, indleft, indright, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_ind1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mdf_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mright_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmean_right_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tree_dist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1952\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m             return self.obj._reindex_with_indexers(\n\u001b[1;32m   1597\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 1655\u001b[0;31m                     \u001b[0;34m\"Passing list-likes to .loc or [] with any missing labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m                     \u001b[0;34m\"is no longer supported, see \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m                     \u001b[0;34m\"https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\"\u001b[0m  \u001b[0;31m# noqa:E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Passing list-likes to .loc or [] with any missing labels is no longer supported, see https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike'"
     ]
    }
   ],
   "source": [
    "for fn in preprocs:\n",
    "    num_rows = fn[0].shape[0] + fn[1].shape[0]\n",
    "    num_dim = fn[0].shape[1]\n",
    "    \n",
    "    for depth in range(1, 6):\n",
    "        start = time.time()\n",
    "        tree = get_tree_clust(depth, fn)\n",
    "        end = time.time()\n",
    "        num_rows_lst.append(num_rows)\n",
    "        num_dim_lst.append(num_dim)\n",
    "        depth_lst.append(depth)\n",
    "        time_lst.append(end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['num_rows', 'num_cols', 'tree_depth', 'time'])\n",
    "df['num_rows'] = num_rows_lst\n",
    "df['num_cols'] = num_dim_lst\n",
    "df['tree_depth'] = depth_lst\n",
    "df['time'] = time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-dbd5cfc8c724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_tree_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_processing_adult2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnum_rows_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_dim_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdepth_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_rows' is not defined"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tree = calc_tree_dist(5, pre_processing_adult2())\n",
    "end = time.time()\n",
    "# num_rows_lst.append(num_rows)\n",
    "# num_dim_lst.append(num_dim)\n",
    "# depth_lst.append(depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7254.31316781044\n"
     ]
    }
   ],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
