{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tree Clusters Pipeline\n",
    "\n",
    "This code will build the supervised tree clusters to minimize distance between samples in each node. In order to run this code you will need to: \n",
    "1. Create your own pre_process function to read in your dataset and return the following dataframes: xTrain, xTest, yTrain, yTest, df_train, df_test (the last 2 are the full dataframes for training and testing) \n",
    "2. Run all cells below \n",
    "3. Call get_tree_clust(depth, pre_process_func) on the depth you want for your tree and the pre_process function that you just created. Your resulting tree will be printed and we will return the ROC AUC score for the tree and the resulting tree itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/aparnacalambur/Documents/Cornell/Classes/MEng/interpretable_clustering', '', '/Library/Python/3.6/site-packages', '/Users/aparnacalambur/Documents/Cornell/Classes/MEng/interpretable_clustering', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '/Users/aparnacalambur/Library/Python/3.7/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/extensions', '/Users/aparnacalambur/.ipython', '/Users/aparnacalambur/Documents/Cornell']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import scipy.stats as sc\n",
    "# import shap\n",
    "# import lime\n",
    "import sklearn \n",
    "import warnings\n",
    "#import xgboost\n",
    "import itertools\n",
    "import gc\n",
    "import networkx as nx\n",
    "import pydot\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "from itertools import product \n",
    "import time\n",
    "import pickle \n",
    "#import interpret\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "# from interpret.glassbox import ExplainableBoostingClassifier\n",
    "# from interpret import show\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from interpret.glassbox import ExplainableBoostingRegressor\n",
    "# from interpret import show\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "#import statsmodels.api as sm\n",
    "sys.path.append(os.path.abspath(\"../../../\"))\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "sys.path.append(\"/anaconda3/lib/python3.6/site-packages\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def pre_process(ts=0.3):\n",
    "    titanic = pd.read_csv('train.csv')\n",
    "    #titanic\n",
    "    full_data = titanic\n",
    "    full_data = full_data.drop(['PassengerId'], axis=1)\n",
    "\n",
    "    deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "    full_data['Cabin'] = full_data['Cabin'].fillna(\"U0\")\n",
    "    full_data['Deck'] = full_data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    full_data['Deck'] = full_data['Deck'].map(deck)\n",
    "    full_data['Deck'] = full_data['Deck'].fillna(0)\n",
    "    full_data['Deck'] = full_data['Deck'].astype(int)\n",
    "\n",
    "    full_data = full_data.drop('Cabin', axis = 1)\n",
    "\n",
    "    mean = full_data[\"Age\"].mean()\n",
    "    std = full_data[\"Age\"].std()\n",
    "    is_null = full_data[\"Age\"].isnull().sum()\n",
    "    # compute random numbers between the mean, std and is_null\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = full_data[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    full_data[\"Age\"] = age_slice\n",
    "    full_data[\"Age\"] = full_data[\"Age\"].astype(int)\n",
    "    full_data[\"Age\"].isnull().sum()\n",
    "    full_data['Embarked'] = full_data['Embarked'].fillna('S')\n",
    "\n",
    "\n",
    "    full_data['Fare'] = full_data['Fare'].fillna(0)\n",
    "    full_data['Fare'] = full_data['Fare'].astype(int)\n",
    "    full_data = full_data.drop(['Name'], axis=1)\n",
    "    full_data = full_data.drop(['Ticket'], axis=1)\n",
    "    full_data['Sex'] = full_data['Sex'].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = ts,random_state = 10)\n",
    "\n",
    "    full_data = pd.get_dummies(full_data,columns = ['Pclass','Embarked','Deck'])\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('Survived',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['Survived']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = ts,random_state = 10)\n",
    "    full_data_train = full_data_train[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    full_data_test = full_data_test[[\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Deck\"]]\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_proc_synthetic_non_linear():\n",
    "    n = 300\n",
    "    X = np.random.randn(n, 10)\n",
    "    y = -2 * np.sin(2*X[:,0]*X[:,2] ) + np.maximum(X[:,1], 0)  + np.exp(-X[:,3]) + np.random.randn(n)\n",
    "    cols = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
    "    X = preprocessing.scale(X)\n",
    "    y = preprocessing.scale(y)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    c = ['y'] + cols\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    full_data['y'] = y\n",
    "    for col in cols:\n",
    "        full_data[col] = X[col]\n",
    "    \n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train, full_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=1000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult2():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=2500, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult3():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=5000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult4():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=10000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult5():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=20000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_adult6():\n",
    "    income_data = pd.read_csv('adult.csv')\n",
    "    inc_no_tar = income_data.drop(columns = ['income'])\n",
    "    c = ['income'] + list(inc_no_tar.columns)\n",
    "    data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        data[col] = income_data[col]\n",
    "    income_data = data.copy()\n",
    "\n",
    "    income_data['workclass']= income_data['workclass'].replace({\"?\":\"Unknown\"})\n",
    "    income_data['native-country']= income_data['native-country'].replace({\"?\":\"Unknown\"})\n",
    "    income_data = income_data.drop(['education', 'marital-status', 'fnlwgt'], axis=1)\n",
    "    \n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    income_data['workclass']= label_encoder.fit_transform(income_data['workclass'])\n",
    "    income_data['occupation']= label_encoder.fit_transform(income_data['occupation'])\n",
    "    income_data['relationship']= label_encoder.fit_transform(income_data['relationship'])\n",
    "    income_data['race']= label_encoder.fit_transform(income_data['race'])\n",
    "    income_data['gender']= label_encoder.fit_transform(income_data['gender'])\n",
    "    income_data['native-country']= label_encoder.fit_transform(income_data['native-country'])\n",
    "    income_data['income']= label_encoder.fit_transform(income_data['income'])\n",
    "    \n",
    "    full_data = income_data.sample(n=40000, random_state=1)\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('income',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['income']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_heart():\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    #full_data = heart\n",
    "    heart = pd.read_csv(\"heart.csv\")\n",
    "    heart_no_tar = heart.drop(columns = ['target'])\n",
    "    c = ['target'] + list(heart_no_tar.columns)\n",
    "    full_data = pd.DataFrame(columns=c)\n",
    "    for col in c:\n",
    "        full_data[col] = heart[col]\n",
    "\n",
    "    full_data_train,full_data_test = train_test_split(full_data,test_size = 0.3,random_state = 10)\n",
    "\n",
    "    full_data = full_data.dropna().reset_index().drop('index',axis = 1)\n",
    "    X = full_data.drop('target',axis = 1)\n",
    "    cols = X.columns\n",
    "    y = full_data['target']\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X =pd.DataFrame(X, columns = cols)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X,y, test_size = 0.3,random_state = 10)\n",
    "    return xTrain, xTest, yTrain, yTest, full_data_train,full_data_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/dev/auto_examples/tree/plot_unveil_tree_structure.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall(distance):\n",
    "    nV = len(distance)\n",
    "    for k in range(nV):\n",
    "        for i in range(nV):\n",
    "            for j in range(nV):\n",
    "                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])\n",
    "                #n += 1\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_dists(estimator):\n",
    "    n_nodes = estimator.tree_.node_count\n",
    "    children_left = estimator.tree_.children_left\n",
    "    children_right = estimator.tree_.children_right\n",
    "\n",
    "    dists = np.empty((n_nodes,n_nodes)); dists.fill(10000)\n",
    "    for i in range(len(children_left)):\n",
    "        left_node_id = children_left[i]\n",
    "        if left_node_id != -1:\n",
    "            dists[i][left_node_id] = 1\n",
    "            dists[left_node_id][i] = 1\n",
    "\n",
    "    for i in range(len(children_right)):\n",
    "        if children_right[i] != -1:\n",
    "            dists[i][children_right[i]] = 1\n",
    "            dists[children_right[i]][i] = 1\n",
    "    \n",
    "    return dists, n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_nodes(xTrain,yTrain,xTest, md, num_trees, leaves):\n",
    "    train = xTrain.copy()\n",
    "    train['y'] = yTrain\n",
    "    train1 = train.sample(n = len(train), replace = True) \n",
    "    train1=train1.dropna(how='any')\n",
    "    yTrain1 = train1['y']\n",
    "    xTrain1 = train1.drop('y',axis = 1)\n",
    "    gc.collect()\n",
    "    \n",
    "    for i in range(num_trees):\n",
    "        estimator = DecisionTreeClassifier(max_depth=md).fit(xTrain1,yTrain1)\n",
    "        leaves_train = estimator.apply(xTrain)\n",
    "        for i in range(len(leaves_train)):\n",
    "            leaves[i].append(leaves_train[i])\n",
    "    \n",
    "    leaves = [tuple(i) for i in leaves]\n",
    "    inds= [leaves.index(x) for x in set(leaves)]\n",
    "    inds.sort()\n",
    "    reps = [0]*(len(leaves))\n",
    "    for i in range(len(leaves)):\n",
    "        leaf_tup = leaves[i]\n",
    "        for ind in inds:\n",
    "            if leaf_tup == leaves[ind]:\n",
    "                reps[i] = ind\n",
    "    \n",
    "    pairs = []\n",
    "    for i in range(len(inds)):\n",
    "        for j in range(i+1, len(inds)):\n",
    "            pairs.append((inds[i], inds[j])) \n",
    "        \n",
    "    return pairs, inds, reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_fw(xTrain,yTrain,xTest, md, pairs, inds):\n",
    "    train = xTrain.copy()\n",
    "    train['y'] = yTrain\n",
    "    train1 = train.sample(n = len(train), replace = True) \n",
    "    train1=train1.dropna(how='any')\n",
    "    yTrain1 = train1['y']\n",
    "    xTrain1 = train1.drop('y',axis = 1)\n",
    "    gc.collect()\n",
    "    \n",
    "    estimator = DecisionTreeClassifier(max_depth=md).fit(xTrain1,yTrain1)\n",
    "    leaves_train = estimator.apply(xTrain1)\n",
    "    graph, n = get_tree_dists(estimator)\n",
    "    fw_dist = floyd_warshall(graph)\n",
    "    \n",
    "    i1_train = [i for i, _ in pairs]\n",
    "    i2_train = [i for _, i in pairs]\n",
    "    #print(fw_dist.shape)\n",
    "    #print(leaves_train)\n",
    "    train_dists = [fw_dist[leaves_train[i]][leaves_train[j]] for i, j in pairs]\n",
    "    train_dist_df = pd.DataFrame(i1_train,columns=['i1'])\n",
    "    train_dist_df['i2'] = i2_train\n",
    "    train_dist_df['tree_dist'] = train_dists\n",
    "    \n",
    "    return([estimator,train_dist_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_random_forest(xTrain,yTrain,num_trees,xTest, md, pairs, inds):\n",
    "    i = 0\n",
    "    mods = []\n",
    "    train_dists = pd.DataFrame()\n",
    "    while i <= num_trees:\n",
    "        tree = build_tree_fw(xTrain,yTrain,xTest, md, pairs, inds)\n",
    "        mods.append(tree[0])\n",
    "        train_dists = train_dists.append(tree[1])\n",
    "        i = i+1\n",
    "        \n",
    "    train_final_dist = train_dists.groupby(['i1','i2']).mean().reset_index()\n",
    "    return(mods,train_final_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(xTest,mods):\n",
    "    pred = []\n",
    "    for clf in mods:\n",
    "        pred.append(clf.predict(xTest))\n",
    "    pred = np.mean(pred,axis = 0)\n",
    "    pred = [int(x) for x in pred>=0.5]\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset, df):\n",
    "    feature = df.columns[index]\n",
    "    left, right = list(), list()\n",
    "    indleft, indright = list(), list()\n",
    "    left_df = df[df[feature] < value]\n",
    "    right_df = df[df[feature] >= value]\n",
    "    for i, row in enumerate(dataset):\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "            indleft.append(i)\n",
    "        else:\n",
    "            right.append(row)\n",
    "            indright.append(i)\n",
    "    #print(indleft)\n",
    "    return left, right, indleft, indright, left_df, right_df\n",
    " \n",
    "# Calculate the avg distance for a split dataset\n",
    "def distance_index(indleft, indright, reps, train_final_dist_dct):\n",
    "    left_len = len(indleft)\n",
    "    right_len = len(indright)\n",
    "    mean_left_dist = 0\n",
    "    mean_right_dist = 0\n",
    "    if left_len > 0:\n",
    "        left_nodes_map = []\n",
    "        for i in indleft:\n",
    "            left_nodes_map.append(reps[i])\n",
    "        left_reps = list(set(left_nodes_map))\n",
    "        left_reps.sort()\n",
    "        left_pairs = []\n",
    "        for i in range(len(left_reps)):\n",
    "            for j in range(i+1, len(left_reps)):\n",
    "                left_pairs.append((left_reps[i], left_reps[j]))\n",
    "        \n",
    "        c = Counter(left_nodes_map)\n",
    "        for pair in left_pairs:\n",
    "            mean_left_dist += c[pair[0]] * c[pair[1]] * train_final_dist_dct[pair]\n",
    "        \n",
    "    if right_len > 0:\n",
    "        right_nodes_map = []\n",
    "        for i in indright:\n",
    "            right_nodes_map.append(reps[i])\n",
    "        right_reps = list(set(right_nodes_map))\n",
    "        right_reps.sort()\n",
    "        right_pairs = []\n",
    "        for i in range(len(right_reps)):\n",
    "            for j in range(i+1, len(right_reps)):\n",
    "                right_pairs.append((right_reps[i], right_reps[j]))\n",
    "        \n",
    "        c = Counter(right_nodes_map)\n",
    "        for pair in right_pairs:\n",
    "            mean_right_dist += c[pair[0]] * c[pair[1]] * train_final_dist_dct[pair]\n",
    "\n",
    "    \n",
    "    left_avg = (mean_left_dist / left_len) * (left_len / (left_len + right_len)) if left_len > 0 else 0\n",
    "    right_avg = (mean_right_dist / right_len) * (right_len / (left_len + right_len)) if right_len > 0 else 0\n",
    "    \n",
    "    return left_avg + right_avg\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, df, reps, train_final_dist_dct):\n",
    "    class_values = list(set(row[0] for row in dataset))\n",
    "    s = 0\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(1, len(dataset[0])):\n",
    "        for row in dataset:\n",
    "            s+= 1\n",
    "            left, right, indleft, indright, left_df, right_df = test_split(index, row[index], dataset, df)\n",
    "            groups = left, right, left_df, right_df\n",
    "            dist = distance_index(indleft, indright, reps, train_final_dist_dct)\n",
    "            if dist < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], dist, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "# Create a terminal node value\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[0] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    " \n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth, reps, train_final_dist_dct):\n",
    "    left, right, left_df, right_df = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        #print('here')\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, left_df, reps, train_final_dist_dct)\n",
    "        split(node['left'], max_depth, min_size, depth+1, reps, train_final_dist_dct)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        #print('here')\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        #print('here')\n",
    "        node['right'] = get_split(right, right_df, reps, train_final_dist_dct)\n",
    "        split(node['right'], max_depth, min_size, depth+1, reps, train_final_dist_dct)\n",
    "# Build a decision tree\n",
    "def build_tree(train, df, max_depth, min_size, reps, train_final_dist_dct):\n",
    "    root = get_split(train, df, reps, train_final_dist_dct)\n",
    "    split(root, max_depth, min_size, 1, reps, train_final_dist_dct)\n",
    "    return root\n",
    " \n",
    "# Print a decision tree\n",
    "def print_tree(node, df, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[%s < %.3f]' % ((depth*' ', (df.columns[node['index']]), node['value'])))\n",
    "        print_tree(node['left'], df, depth+1)\n",
    "        print_tree(node['right'], df, depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))\n",
    "        \n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_lst(tree_full, dataset_test):\n",
    "    y_pred_dt = []\n",
    "    for row in dataset_test:\n",
    "        y_pred_dt.append(predict(tree_full, row))\n",
    "    return y_pred_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Distance Calc\n",
    "Defne your own pre_process() function to return the dataframes: xTrain, xTest, yTrain, yTest, df_train, and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_lst = []\n",
    "num_dim_lst = []\n",
    "depth_lst = []\n",
    "time_lst = []\n",
    "num_nodes = []\n",
    "#mean_nodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocs = [pre_process(), pre_processing_heart(), pre_processing_adult(), pre_processing_adult2(), pre_processing_adult3(), pre_processing_adult4(), pre_processing_adult5(), pre_processing_adult6()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "New Dataset\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n",
      "new depth\n"
     ]
    }
   ],
   "source": [
    "for fn in preprocs:\n",
    "    print(\"New Dataset\")\n",
    "    num_rows = fn[0].shape[0] + fn[1].shape[0]\n",
    "    num_dim = fn[0].shape[1]\n",
    "    \n",
    "    for depth in range(1, 6):\n",
    "        print(\"new depth\")\n",
    "        xTrain, xTest, yTrain, yTest, df_train, df_test = fn\n",
    "        dataset_train = df_train.to_numpy()\n",
    "        dataset_test = df_test.to_numpy()\n",
    "        rep_nodes = [[] for _ in range(len(df_train))]\n",
    "        start = time.time()\n",
    "        pairs, inds, reps = get_rep_nodes(xTrain,yTrain,xTest, depth, 50, rep_nodes)\n",
    "        mods,train_final_dist= fit_random_forest(xTrain,yTrain,50,xTest, depth, pairs, inds)\n",
    "        end = time.time()\n",
    "        num_nodes.append(len(inds))\n",
    "        num_rows_lst.append(num_rows)\n",
    "        num_dim_lst.append(num_dim)\n",
    "        depth_lst.append(depth)\n",
    "        time_lst.append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['num_rows', 'num_cols', 'tree_depth', 'num_nodes', 'time'])\n",
    "df['num_rows'] = num_rows_lst\n",
    "df['num_cols'] = num_dim_lst\n",
    "df['tree_depth'] = depth_lst\n",
    "df['num_nodes'] = num_nodes\n",
    "df['time'] = time_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('rep_nodes_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Cluster Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the list of nodes to translate them into rep nodes\n",
    "# count number of times each rep node appears \n",
    "# get a list of pairs of rep nodes  \n",
    "# go through list of pairs, get count of each pair and multiply them together, multiply them to distance\n",
    "# accumulate for each pair, divide by len of ind list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.684985876083374\n"
     ]
    }
   ],
   "source": [
    "xTrain, xTest, yTrain, yTest, df_train, df_test = pre_process()\n",
    "dataset_train = df_train.to_numpy()\n",
    "dataset_test = df_test.to_numpy()\n",
    "leaves = [[] for _ in range(len(df_train))]\n",
    "pairs, inds, reps = get_rep_nodes(xTrain,yTrain,xTest, 5, 50, leaves)\n",
    "mods,train_final_dist= fit_random_forest(xTrain,yTrain,50,xTest, 5, pairs, inds)\n",
    "subset = train_final_dist[['i1', 'i2']]\n",
    "tuples = [tuple(x) for x in subset.to_numpy()]\n",
    "dist = dict(zip(tuples, train_final_dist['tree_dist'])) \n",
    "#build_tree(dataset_train, df_train, 4, 10, reps, dist)\n",
    "start = time.time()\n",
    "t = build_tree(dataset_train, df_train, 5, 30, reps, dist)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
